# -*- coding: utf-8 -*-
"""Keyword_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q7OKvt74831BzPsoqJNytEsmuF1sbljZ

# LLM Based Keywording
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

# %cd /content/drive/MyDrive/CS/프로젝트/LLM Innovators Challenge

"""## Prompting"""

import re

def keywording_prompt():
  prompt_template = """
        # 맥락 정보 #
        너는 리뷰에 대한 키워드를 만드는 일의 전문가야.
        사용자가 긴 리뷰를 다 읽는데 어려움을 겪고 있어서, 너가 리뷰의 내용을 대표할 만한 핵심 단어들로 키워드로 만드는 일을 하고 있어.

        # 지시사항 #
        지금부터 호텔의 리뷰를 보고, 키워드를 만들거야.
        리뷰인 {review}를 읽고, 아래 3단계에 거쳐 10개 이상 30개 이하의 키워드를 생각해 봐.
        1단계. 리뷰를 대표하는 단어로는 어떤 게 있는가?
        2단계. 리뷰가 긍정, 부정에 관한 내용을 포함하는가? 포함한다면, 해당 내용을 키워드로 만들어라.
        3단계. 리뷰에 특이사항이 있는가? 있다면, 해당 내용을 키워드로 만들어라.

        # 출력 형식 #
        keyword1, keyword2, ... 에는 너가 생성한 키워드를 넣어서 반환하면 돼.
        반드시 아래와 같은 형식을 엄격히 지켜주고, 키워드를 반환해야해.
        그리고 반드시 10개 이상 30개 이하여야 해.
        "keywords": ["keyword1", "keyword2", ...]

        # 예시 #
        리뷰:
        "음식이 너무 맛있고, 서비스도 훌륭했습니다."
        출력:
        "keywords": ["맛있다", "서비스 훌륭"]
    """
  return prompt_template

def parser(response):
  # 키워드 리스트를 저장할 리스트
  match1 = re.search(r'"?keywords"?: \[(.*?)\]', response)  # "keywords" 또는 keywords
  match2 = re.search(r'"?키워드"?: \[(.*?)\]', response)   # "키워드" 또는 키워드


  if match1:
      # 매치된 그룹에서 키워드 문자열 추출
      keywords_str = match1.group(1)
      # 문자열을 리스트로 변환하고 앞뒤 공백 및 따옴표 제거
      keywords_list = [keyword.strip().strip('"') for keyword in keywords_str.split(',')]
      return keywords_list
  elif match2:
      # 매치된 그룹에서 키워드 문자열 추출
      keywords_str = match2.group(1)
      # 문자열을 리스트로 변환하고 앞뒤 공백 및 따옴표 제거
      keywords_list = [keyword.strip().strip('"') for keyword in keywords_str.split(',')]
      return keywords_list
  else:
      raise ValueError("No keywords found in content.")

# 예시 문자열
content = '''```json
"keywords": ["뒷고기", "맛집", "비빔냉면", "친절", "주차"]
```'''
parser(content)

"""## Gemini"""

import getpass
import os
import time
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
import pandas as pd

!pip install -q -U pip
!pip install -q -U langchain-google-genai
!pip install -q -U langchain

def gemini_api():
  if "GOOGLE_API_KEY" not in os.environ:
      os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google AI API key: ")

def review_keywording(file_path):
  df = pd.read_csv(file_path)
  df_review = df["reviews"]
  df["keywords"] = None

  index_input = input("저장할 인덱스 번호를 입력하세요: ")
  index = int(index_input)

  gemini_api()

  prompt_template = keywording_prompt()
  prompt = ChatPromptTemplate.from_messages(
      [
          ("system", prompt_template),
          ("human", "{review}")
      ]
  )

  llm = ChatGoogleGenerativeAI(
      model="gemini-1.5-pro",
      temperature=0.2,
      timeout=None,
      max_retries=2,
      top_p = 0.1,
      # other params...
  )

  chain = prompt | llm

  for i in range(index, len(df)):
    print("*", i+1, "번째", df["restaurant_name"][i])
    for j in range(len(eval(df_review.iloc[i]))):  # eval로 리스트 변환
        print(j+1, "번째 리뷰")

        response = chain.invoke({"review": eval(df_review.iloc[i])[j]}).content
        response = parser(response)

        print("keyword: ", response)
        print("="*50)

        if(j%10 == 0):
          time.sleep(60)
        else:
          time.sleep(20)

    df.at[i, "keywords"] = response
    df.to_csv('./food_review_keyword', index=False)

    print(df)
    print()
    print("="*50)

review_keywording('./food_review_pre.csv')

"""## Solar"""

!pip install -q -U pip
!pip install -q -U langchain
!pip install -q -U langchain-core
!pip install -U langchain-upstage

import os
import getpass

def solar_api():
  if "SOLAR_API_KEY" not in os.environ:
      os.environ["SOLAR_API_KEY"] = getpass.getpass("Enter your Solar API key: ")

import pandas as pd
import ast
import time
import json
from langchain_upstage import ChatUpstage
from langchain.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import LLMChain


def review_keywording(file_path):
    # Load data from CSV
    df = pd.read_csv(file_path)
    df_review = df["reviews"]
    df["keywords"] = None

    # Ask the user for the index from which to start
    index_input = input("저장할 인덱스 번호를 입력하세요: ")
    index = int(index_input)

    solar_api()

    # Initialize the Solar model
    llm = ChatUpstage(api_key=os.getenv("SOLAR_API_KEY"), model="solar-1-mini-chat")

    # Define the prompt template
    prompt_template = keywording_prompt()
    prompt = ChatPromptTemplate.from_messages(
                  [
                      ("system", prompt_template),
                      ("human", "{review}")
                  ]
              )
    chain = prompt | llm

    # Bind the tools (e.g., keyword parser)
    tools = [keyword_parser]
    llm_with_tools = llm.bind_tools(tools)

    # Loop through the data and extract keywords
    for i in range(index, len(df)):
        print(f"* {i+1}번째", df["restaurant_name"][i])

        # Convert review string to a list using safer ast.literal_eval
        reviews = ast.literal_eval(df_review.iloc[i])

        keywords_list = []

        for j in range(len(reviews)):  # reviews는 리스트 형태
            review = reviews[j]  # 리뷰에 접근

            print(f"{j+1}번째 리뷰: {review}")
            response = chain.invoke({"review": review}).content
            print(response)
            response_parsed = parser(response)
            print(response_parsed)
            print("="*50)

            # Add delay to avoid overloading the API
            if j % 10 == 0:
                time.sleep(60)
            else:
                time.sleep(20)

        # Save the extracted keywords back to the DataFrame
        df.at[i, "keywords"] = str(keywords_list)
        df.to_csv('./food_review_keyword.csv', index=False)

        print(df)
        print("="*50)

review_keywording('./food_review_pre.csv')

import pandas as pd
import ast
import time
from langchain_upstage import ChatUpstage
from langchain.tools import tool
from langchain_core.prompts import ChatPromptTemplate

def review_keywording_batch(file_path):
    # Load data from CSV
    df = pd.read_csv(file_path)
    df_review = df["reviews"]
    df["keywords"] = None

    # Ask the user for the index from which to start
    index_input = input("저장할 인덱스 번호를 입력하세요: ")
    index = int(index_input)

    solar_api()

    # Initialize the Solar model
    llm = ChatUpstage(api_key=os.getenv("SOLAR_API_KEY"), model="solar-1-mini-chat")

    # Define the prompt template
    prompt_template = keywording_prompt()
    prompt = ChatPromptTemplate.from_messages(
                  [
                      ("system", prompt_template),
                      ("human", "{review}")
                  ]
              )
    chain = prompt | llm

    # Loop through the data and extract keywords
    for i in range(index, len(df)):
        print(f"* {i+1}번째", df["restaurant_name"][i])

        # Convert review string to a list using safer ast.literal_eval
        reviews = ast.literal_eval(df_review.iloc[i])

        # Create a list to store keywords for this restaurant
        keywords_list = []

        # Process reviews in batches of 10
        for batch_start in range(0, len(reviews), 10):
            batch_reviews = reviews[batch_start:batch_start + 10]
            print(f"Batch {batch_start // 10 + 1} processing: {batch_reviews}")

            # Extract keywords for each review in the batch
            for review in batch_reviews:
                print(f"Processing review: {review}")
                response = chain.invoke({"review": review}).content
                print(response)
                response_parsed = parser(response)
                print("Parsed keywords:", response_parsed)

                # Append the parsed keywords to the keywords_list
                keywords_list.append(response_parsed)

                print("=" * 50)

                # Add delay to avoid overloading the API
                time.sleep(20)

        # Save the extracted keywords back to the DataFrame
        df.at[i, "keywords"] = str(keywords_list)
        df.to_csv('./food_review_keyword.csv', index=False)

        print(df)
        print("=" * 50)

review_keywording_batch('./food_review_pre.csv')

import pandas as pd
import ast
import time
from langchain_upstage import ChatUpstage
from langchain.tools import tool
from langchain_core.prompts import ChatPromptTemplate

def long_review_keywording(file_path):
    # Load data from CSV
    df = pd.read_csv(file_path)
    df_review = df["reviews"]
    df["keywords"] = None

    # Ask the user for the index from which to start
    index_input = input("저장할 인덱스 번호를 입력하세요: ")
    index = int(index_input)

    solar_api()

    # Initialize the Solar model
    llm = ChatUpstage(api_key=os.getenv("SOLAR_API_KEY"), model="solar-1-mini-chat")

    # Define the prompt template
    prompt_template = keywording_prompt()
    prompt = ChatPromptTemplate.from_messages(
                  [
                      ("system", prompt_template),
                      ("human", "{review}")
                  ]
              )
    chain = prompt | llm

    # Loop through the data and extract keywords
    for i in range(index, len(df)):
        print(f"* {i+1}번째", df["restaurant_name"][i])

        # Convert review string to a list using safer ast.literal_eval
        reviews = ast.literal_eval(df_review.iloc[i])

        # Create a list to store keywords for this restaurant
        keywords_list = []

        # Process reviews in batches of 10 by concatenating them
        for batch_start in range(0, len(reviews), 10):
            # Concatenate the reviews from batch_start to batch_start + 10 into a single string
            batch_reviews = " ".join(reviews[batch_start:batch_start + 10])
            print(f"Processing concatenated batch {batch_start // 10 + 1}: {batch_reviews}")

            # Send the concatenated reviews to LLM for keyword extraction
            response = chain.invoke({"review": batch_reviews}).content
            print(response)
            response_parsed = parser(response)
            print("Parsed keywords:", response_parsed)

            # Append the parsed keywords to the keywords_list
            keywords_list.append(response_parsed)

            print("=" * 50)

            # Add delay to avoid overloading the API
            time.sleep(20)

        # Save the extracted keywords back to the DataFrame
        df.at[i, "keywords"] = str(keywords_list)
        df.to_csv('./food_lreview_keyword.csv', index=False)

        print(df)
        print("=" * 50)

long_review_keywording('./food_review_pre.csv')

"""### LLM Compressor"""

def summerize_prompt():
  prompt_template = """
        # 맥락 정보 #
        너는 리뷰를 요약하는 일의 한국인 전문가야.
        # 지시사항 #
        지금부터 숙소의 리뷰인 {review}를 보고, 다섯 줄로 요약할 거야.
        너가 생각했을 때 중요한 내용을 위주로 요약해줘.
        요약된 내용은 반드시 리뷰의 내용을 대표할 수 있어야 해.
        # 출력 형식 #
        반드시 한국어 문장의 형태로 출력해.
    """
  return prompt_template

def summerize_review(review, llm):
  # Convert review string to a list using safer ast.literal_eval
  # Concatenate reviews into a single string
  combined_reviews = " ".join(review)

  prompt_template = summerize_prompt()
  prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", prompt_template),
                    ("human", "{review}")
                ]
            )
  chain = prompt | llm
  response = chain.invoke({"review": combined_reviews}).content
  print()
  print("summerized :", response)
  print()
  return response

import pandas as pd
import ast
import time
from langchain_upstage import ChatUpstage
from langchain_core.prompts import ChatPromptTemplate

def s_review_keywording(file_path):
    # Load data from CSV
    df = pd.read_csv(file_path)
    df_review = df["reviews"]
    df["keywords"] = None

    # Ask the user for the index from which to start
    index_input = input("저장할 인덱스 번호를 입력하세요: ")
    index = int(index_input)

    end_index = int(input("끝 인덱스 번호를 입력하세요: "))

    solar_api()

    # Initialize the Solar model
    llm = ChatUpstage(api_key=os.getenv("SOLAR_API_KEY"), model="solar-1-mini-chat")

    # Define the prompt template
    prompt_template = keywording_prompt()
    prompt = ChatPromptTemplate.from_messages(
                  [
                      ("system", prompt_template),
                      ("human", "{review}")
                  ]
              )
    chain = prompt | llm

    # Loop through the data and extract keywords
    for i in range(index, end_index):
        print(f"* {i+1}번째", df["hotel_name"][i])

        # Convert review string to a list using safer ast.literal_eval
        reviews = ast.literal_eval(df_review.iloc[i])

        summerized_review = summerize_review(reviews, llm)
        df.at[i, "summerized_review"] = summerized_review

        # Send the combined reviews to LLM for keyword extraction
        response = chain.invoke({"review": summerized_review}).content
        print(response)
        response_parsed = parser(response)
        print("Parsed keywords:", response_parsed)

        # Save the extracted keywords back to the DataFrame
        df.at[i, "keywords"] = str(response_parsed)
        df.to_csv('./hotel_sreview_keyword.csv', index=False)

        #print(df)
        print("=" * 50)

        # Add delay to avoid overloading the API
        time.sleep(20)

s_review_keywording('./hotel_review_pre.csv')

"""### Concat File"""

import pandas as pd

# CSV 파일 읽기
file1 = pd.read_csv('./cafe_sreview_keyword1.csv')
file2 = pd.read_csv('./cafe_sreview_keyword2.csv')

# 파일 1의 첫 번째 110행 + 파일 2의 나머지 행(111행부터 끝까지)
merged_file = pd.concat([file1.iloc[:230], file2[231:]], ignore_index=True)

# 결과를 새로운 CSV 파일로 저장
merged_file.to_csv('./cafe_sreview_keyword.csv', index=False)



"""## COSP : Final Keywording

## ROUGE-L
"""

!pip install rouge-score

!pip install rouge

from rouge import Rouge

# 기준 텍스트와 생성된 텍스트
reference = "이것은 기준 텍스트입니다. 이 텍스트는 요약의 품질을 평가하기 위한 것입니다."
generated = "이 텍스트는 요약 품질 평가를 위한 기준입니다."

# ROUGE-L 계산
rouge = Rouge()
rouge_scores = rouge.get_scores(generated, reference, avg=True)
print(rouge_scores)
rouge_l_fmeasure = rouge_scores["rouge-l"]["f"]

print(f"ROUGE-L F-measure: {rouge_l_fmeasure}")

from rouge import Rouge

def rougeL(reference, generated):
    rouge = Rouge()
    rouge_scores = rouge.get_scores(generated, reference, avg=True)
    print(rouge_scores)

    rouge_l_fmeasure = rouge_scores["rouge-l"]

    return rouge_l_fmeasure

from rouge import Rouge

def rouge1(reference, generated):
    rouge = Rouge()
    rouge_scores = rouge.get_scores(generated, reference, avg=True)
    #print(rouge_scores)

    rouge_1_fmeasure = rouge_scores["rouge-1"]

    return rouge_1_fmeasure

from rouge import Rouge

def rouge2(reference, generated):
    rouge = Rouge()
    rouge_scores = rouge.get_scores(generated, reference, avg=True)
    #print(rouge_scores)

    rouge_2_fmeasure = rouge_scores["rouge-2"]

    return rouge_2_fmeasure

def summarize_rougeL(file_path):
    # Load data from CSV
    df = pd.read_csv(file_path)

    df = df.drop(index=210)
    df = df.reset_index(drop=True)

    df_review = df["reviews"]
    df_summarize = df["summerized_review"]

    rl_fmeasure_mean = 0
    r1_fmeasure_mean = 0
    r2_fmeasure_mean = 0

    rl_pmeasure_mean = 0
    r1_pmeasure_mean = 0
    r2_pmeasure_mean = 0

    rl_rmeasure_mean = 0
    r1_rmeasure_mean = 0
    r2_rmeasure_mean = 0

    for i in range(len(df)):
        print(i, df['restaurant_name'][i])
        reviews = ast.literal_eval(df_review.iloc[i])
        combined_reviews = " ".join(reviews)
        df.at[i, "total_review"] = combined_reviews

        r1_measure = rouge1(combined_reviews, df_summarize.iloc[i])
        r2_measure = rouge2(combined_reviews, df_summarize.iloc[i])
        rl_measure = rougeL(combined_reviews, df_summarize.iloc[i])

        r1_fmeasure = r1_measure["f"]
        r2_fmeasure = r2_measure["f"]
        rl_fmeasure = rl_measure["f"]

        r1_pmeasure = r1_measure["p"]
        r2_pmeasure = r2_measure["p"]
        rl_pmeasure = rl_measure["p"]

        r1_rmeasure = r1_measure["r"]
        r2_rmeasure = r2_measure["r"]
        rl_rmeasure = rl_measure["r"]

        df.loc[i, "r1_fmeasure"] = r1_measure["f"]
        df.loc[i, "r2_fmeasure"] = r2_measure["f"]
        df.loc[i, "rl_fmeasure"] = rl_measure["f"]

        df.loc[i, "r1_pmeasure"] = r1_measure["p"]
        df.loc[i, "r2_pmeasure"] = r2_measure["p"]
        df.loc[i, "rl_pmeasure"] = rl_measure["p"]

        df.loc[i, "r1_pmeasure"] = r1_measure["r"]
        df.loc[i, "r2_pmeasure"] = r2_measure["r"]
        df.loc[i, "rl_pmeasure"] = rl_measure["r"]
        df.to_csv('./food_sreview_keyword_rL.csv', index=False)

        rl_fmeasure_mean += rl_fmeasure
        r1_fmeasure_mean += r1_fmeasure
        r2_fmeasure_mean += r2_fmeasure

        rl_pmeasure_mean += rl_pmeasure
        r1_pmeasure_mean += r1_pmeasure
        r2_pmeasure_mean += r2_pmeasure

        rl_rmeasure_mean += rl_rmeasure
        r1_rmeasure_mean += r1_rmeasure
        r2_rmeasure_mean += r2_rmeasure

        print("f1: ", r1_fmeasure, r2_fmeasure, rl_fmeasure)
        print("precision: ", r1_pmeasure, r2_pmeasure, rl_pmeasure)
        print("recall: ", r1_rmeasure, r2_rmeasure, rl_rmeasure )
        print("="*50)

    rl_fmeasure_mean /= len(df)
    r1_fmeasure_mean /= len(df)
    r2_fmeasure_mean /= len(df)

    rl_rmeasure_mean /= len(df)
    r1_rmeasure_mean /= len(df)
    r2_rmeasure_mean /= len(df)

    rl_pmeasure_mean /= len(df)
    r1_pmeasure_mean /= len(df)
    r2_pmeasure_mean /= len(df)

    print(f"r1_fmeasure_mean : {r1_fmeasure_mean}")
    print(f"r2_fmeasure_mean : {r2_fmeasure_mean}")
    print(f"rl_fmeasure_mean : {rl_fmeasure_mean}")

    print()
    print("="*50)

    print(f"r1_pmeasure_mean : {r1_pmeasure_mean}")
    print(f"r2_pmeasure_mean : {r2_pmeasure_mean}")
    print(f"rl_pmeasure_mean : {rl_pmeasure_mean}")

    print()
    print("="*50)

    print(f"r1_rmeasure_mean : {r1_rmeasure_mean}")
    print(f"r2_rmeasure_mean : {r2_rmeasure_mean}")
    print(f"rl_rmeasure_mean : {rl_rmeasure_mean}")

summarize_rougeL('./food_sreview_keyword.csv')

def keyword_rougeL(file_path):
    # Load data from CSV
    df = pd.read_csv(file_path)

    df = df.drop(index=210)
    df = df.reset_index(drop=True)

    df_keyword = df["keywords"]
    df_summarize = df["summerized_review"]

    rl_fmeasure_mean = 0
    r1_fmeasure_mean = 0
    r2_fmeasure_mean = 0

    rl_pmeasure_mean = 0
    r1_pmeasure_mean = 0
    r2_pmeasure_mean = 0

    rl_rmeasure_mean = 0
    r1_rmeasure_mean = 0
    r2_rmeasure_mean = 0

    for i in range(len(df)):
        print(i, df['restaurant_name'][i])

        keywords = ast.literal_eval(df_keyword.iloc[i])
        combined_keywords = " ".join(keywords)

        df.loc[i, "total_keywords"] = combined_keywords

        r1_measure = rouge1(df_summarize.iloc[i], combined_keywords)
        r2_measure = rouge2(df_summarize.iloc[i], combined_keywords)
        rl_measure = rougeL(df_summarize.iloc[i], combined_keywords)

        r1_fmeasure = r1_measure["f"]
        r2_fmeasure = r2_measure["f"]
        rl_fmeasure = rl_measure["f"]

        r1_pmeasure = r1_measure["p"]
        r2_pmeasure = r2_measure["p"]
        rl_pmeasure = rl_measure["p"]

        r1_rmeasure = r1_measure["r"]
        r2_rmeasure = r2_measure["r"]
        rl_rmeasure = rl_measure["r"]

        df.loc[i, "r1_fmeasure"] = r1_measure["f"]
        df.loc[i, "r2_fmeasure"] = r2_measure["f"]
        df.loc[i, "rl_fmeasure"] = rl_measure["f"]

        df.loc[i, "r1_pmeasure"] = r1_measure["p"]
        df.loc[i, "r2_pmeasure"] = r2_measure["p"]
        df.loc[i, "rl_pmeasure"] = rl_measure["p"]

        df.loc[i, "r1_pmeasure"] = r1_measure["r"]
        df.loc[i, "r2_pmeasure"] = r2_measure["r"]
        df.loc[i, "rl_pmeasure"] = rl_measure["r"]
        df.to_csv('./food_sreview_keyword_rL2.csv', index=False)

        rl_fmeasure_mean += rl_fmeasure
        r1_fmeasure_mean += r1_fmeasure
        r2_fmeasure_mean += r2_fmeasure

        rl_pmeasure_mean += rl_pmeasure
        r1_pmeasure_mean += r1_pmeasure
        r2_pmeasure_mean += r2_pmeasure

        rl_rmeasure_mean += rl_rmeasure
        r1_rmeasure_mean += r1_rmeasure
        r2_rmeasure_mean += r2_rmeasure

        print("f1: ", r1_fmeasure, r2_fmeasure, rl_fmeasure)
        print("precision: ", r1_pmeasure, r2_pmeasure, rl_pmeasure)
        print("recall: ", r1_rmeasure, r2_rmeasure, rl_rmeasure )
        print("="*50)

    rl_fmeasure_mean /= len(df)
    r1_fmeasure_mean /= len(df)
    r2_fmeasure_mean /= len(df)

    rl_rmeasure_mean /= len(df)
    r1_rmeasure_mean /= len(df)
    r2_rmeasure_mean /= len(df)

    rl_pmeasure_mean /= len(df)
    r1_pmeasure_mean /= len(df)
    r2_pmeasure_mean /= len(df)

    print(f"r1_fmeasure_mean : {r1_fmeasure_mean}")
    print(f"r2_fmeasure_mean : {r2_fmeasure_mean}")
    print(f"rl_fmeasure_mean : {rl_fmeasure_mean}")

    print()
    print("="*50)

    print(f"r1_pmeasure_mean : {r1_pmeasure_mean}")
    print(f"r2_pmeasure_mean : {r2_pmeasure_mean}")
    print(f"rl_pmeasure_mean : {rl_pmeasure_mean}")

    print()
    print("="*50)

    print(f"r1_rmeasure_mean : {r1_rmeasure_mean}")
    print(f"r2_rmeasure_mean : {r2_rmeasure_mean}")
    print(f"rl_rmeasure_mean : {rl_rmeasure_mean}")

keyword_rougeL('./food_sreview_keyword.csv')

"""## ROUGE-S"""

from rouge_score import rouge_scorer

# 기준 텍스트와 생성된 텍스트
reference = "이것은 기준 텍스트입니다. 이 텍스트는 요약의 품질을 평가하기 위한 것입니다."
generated = "이 텍스트는 요약 품질 평가를 위한 기준입니다."

# ROUGE-S 계산 (Skip-bigram)
scorer = rouge_scorer.RougeScorer(['rougeS'], use_stemmer=True)
scores = scorer.score(reference, generated)

print(scores)

import numpy as np

def calculate_entropy(probabilities):
    """
    Calculate the conditional entropy H(p | {z}) for given probabilities.
    """
    return -np.sum(probabilities * np.log(probabilities + 1e-10)) / np.log(len(probabilities))

def calculate_similarity(q_a, q_b):
    """
    Calculate cosine similarity between two questions.
    """
    # Normalize the vectors
    norm_a = np.linalg.norm(q_a)
    norm_b = np.linalg.norm(q_b)
    if norm_a == 0 or norm_b == 0:
        return 0.0  # Avoid division by zero
    return np.dot(q_a, q_b) / (norm_a * norm_b)

def calculate_relevance_score(questions):
    """
    Calculate the relevance score R_r(r) based on the questions provided.
    """
    Q = len(questions)
    total_similarity = 0.0

    for a in range(Q):
        for b in range(a + 1, Q):
            total_similarity += calculate_similarity(questions[a], questions[b])

    return (2 * total_similarity) / (Q * (Q - 1))

def cos_p_score(probabilities, questions, lambda_param):
    """
    Calculate the final COSP score.
    """
    # Calculate entropy
    entropy = calculate_entropy(probabilities)

    # Calculate relevance score
    relevance_score = calculate_relevance_score(questions)

    # Calculate final score
    final_score = -entropy - lambda_param * relevance_score

    return final_score

# Example usage
if __name__ == "__main__":
    # Example probabilities for z
    probabilities = np.array([0.2, 0.3, 0.5])  # Example probabilities for z
    # Example questions represented as vectors
    questions = [np.array([1, 0, 1]), np.array([0, 1, 0]), np.array([1, 1, 0])]  # Example question vectors
    lambda_param = 0.5  # Example lambda value

    score = cos_p_score(probabilities, questions, lambda_param)
    print(f"COSP Score: {score}")

"""## Cosine SImilarity"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity as cosine_sim  # sklearn 함수

# 전체 문서와 키워드 정의
document = "이것은 전체 문서의 내용입니다. 문서의 주요 주제를 설명합니다."
keywords = ["주요 주제", "내용", "설명", "문서", "주제", "다른 키워드"]

def calculate_cosine_similarity(document, keywords):

    # TF-IDF 벡터화
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform([document] + keywords)

    # 전체 문서 벡터와 키워드 벡터 분리
    doc_vector = tfidf_matrix[0]  # 전체 문서 벡터
    keyword_vectors = tfidf_matrix[1:]  # 키워드 벡터

    # 코사인 유사도 계산
    cosine_similarities = cosine_sim(doc_vector, keyword_vectors)

    # 유사도와 키워드를 DataFrame으로 변환
    similarity_df = pd.DataFrame(cosine_similarities.T, columns=["similarity"])
    similarity_df["keyword"] = keywords

    # 유사도에 따라 정렬 후 상위 5개 선택
    top_keywords_df = similarity_df.sort_values(by="similarity", ascending=False).head(5)

    # 키워드와 유사도 점수를 리스트로 반환
    top_keywords = top_keywords_df["keyword"].tolist()  # 상위 5개 키워드 리스트
    top_similarities = top_keywords_df["similarity"].tolist()  # 상위 5개 유사도 점수 리스트
    mean_similarity = similarity_df["similarity"].mean()

    print("Top Keywords:", top_keywords)
    print("Similarity Scores:", top_similarities)
    print("Mean Similarity:", mean_similarity)

    return (top_keywords, top_similarities, mean_similarity)

# 함수 호출 및 결과 확인
top_keywords, top_similarities, mean_similarity = calculate_cosine_similarity(document, keywords)

import ast
df = pd.read_csv('./food_sreview_keyword.csv')

for i in range(len(df)):
    print(i, df['restaurant_name'][i])

    df_review = df["reviews"]
    reviews = ast.literal_eval(df_review.iloc[i])
    combined_reviews = " ".join(reviews)

    df.at[i, "total_review"] = combined_reviews

    keywords_list = ast.literal_eval(df["keywords"][i])  # 문자열을 실제 리스트로 변환


    top_keywords, top_similarities, mean_similarity = calculate_cosine_similarity(df['summerized_review'][i], keywords_list)

    if 'top_keywords' not in df.columns:
        df['top_keywords'] = [[] for _ in range(len(df))]

    if 'top_similarities' not in df.columns:
        df['top_similarities'] = [[] for _ in range(len(df))]

    if 'mean_similarities' not in df.columns:
        df['mean_similarities'] = [[] for _ in range(len(df))]

    df.at[i, "top_keywords"] = top_keywords
    df.at[i, "top_similarities"] = top_similarities
    df.loc[i, "mean_similarity"] = mean_similarity

    df.to_csv('./food_sreview_keyword_cosine2.csv', index=False)

!pip install transformers torch

import pandas as pd
import torch
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity

model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir="./models")
model = BertModel.from_pretrained(model_name, cache_dir="./models")

def embed_text(text):
    # 텍스트를 토크나이즈하고 텐서로 변환
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)

    # BERT 모델을 통해 임베딩 계산
    with torch.no_grad():
        outputs = model(**inputs)

    # 임베딩 추출 (여기서는 마지막 은닉 상태의 평균을 사용)
    embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings

def calculate_cosine_similarity2(document, keywords):
    # 전체 문서와 키워드 임베딩 생성
    doc_embedding = embed_text(document)
    keyword_embeddings = [embed_text(keyword) for keyword in keywords]

    # 코사인 유사도 계산
    cosine_similarities = cosine_similarity(doc_embedding.numpy(), torch.cat(keyword_embeddings).numpy())

    # 유사도와 키워드를 DataFrame으로 변환
    similarity_df = pd.DataFrame(cosine_similarities.T, columns=["similarity"])
    similarity_df["keyword"] = keywords

    # 유사도에 따라 정렬 후 상위 5개 선택
    top_keywords_df = similarity_df.sort_values(by="similarity", ascending=False).head(5)

    # 키워드와 유사도 점수를 리스트로 반환
    top_keywords = top_keywords_df["keyword"].tolist()  # 상위 5개 키워드 리스트
    top_similarities = top_keywords_df["similarity"].tolist()  # 상위 5개 유사도 점수 리스트
    mean_similarity = similarity_df["similarity"].mean()  # 전체 평균 유사도

    print("Top Keywords:", top_keywords)
    print("Similarity Scores:", top_similarities)
    print("Mean Similarity:", mean_similarity)

    return (top_keywords, top_similarities, mean_similarity)

# 함수 호출 및 결과 확인
top_keywords, top_similarities, mean_similarity = calculate_cosine_similarity(document, keywords)

import ast
df = pd.read_csv('./food_sreview_keyword.csv')

for i in range(len(df)):
    print(i, df['restaurant_name'][i])

    df_review = df["reviews"]
    reviews = ast.literal_eval(df_review.iloc[i])
    combined_reviews = " ".join(reviews)

    df.at[i, "total_review"] = combined_reviews

    keywords_list = ast.literal_eval(df["keywords"][i])  # 문자열을 실제 리스트로 변환


    top_keywords, top_similarities, mean_similarity = calculate_cosine_similarity2(df['summerized_review'][i], keywords_list)

    if 'top_keywords' not in df.columns:
        df['top_keywords'] = [[] for _ in range(len(df))]

    if 'top_similarities' not in df.columns:
        df['top_similarities'] = [[] for _ in range(len(df))]

    if 'mean_similarities' not in df.columns:
        df['mean_similarities'] = [[] for _ in range(len(df))]

    df.at[i, "top_keywords"] = top_keywords
    df.at[i, "top_similarities"] = top_similarities
    df.loc[i, "mean_similarity"] = mean_similarity

    df.to_csv('./food_sreview_keyword_cosine2-2.csv', index=False)

"""# Fine-tunded LLM Keywording"""